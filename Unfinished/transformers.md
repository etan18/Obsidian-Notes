Transformers are a sophisticated variant of [[neural networks]] that were first introduced in the  landmark paper [Attention is All You Need](https://arxiv.org/abs/1706.03762) published in 2017.

>[!info] Source
>The source material for this page is Andrej Karpathy's YouTube video ["Let's build GPT: from scratch, in code, spelled out"](https://www.youtube.com/watch?v=kCc8FmEb1nY&ab_channel=AndrejKarpathy).
## attention
When we're trying to learn a very specific task (e.g. object identification in an image), we may only need a certain subset of the given features to do so.


## mamba
Mamba is a deep learning architecture 
