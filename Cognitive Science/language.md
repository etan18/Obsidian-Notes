### complexities of language
1.  Relies on _mental representations_
    -   Provides a counterargument to behaviorism
2.  Uniquely human, and humans show an innate basis for language
    -   Provides a counterargument to empiricism
3.  Potentially infinite

The **Sapir-Whorf Hypothesis** of linguistic relativity states that the structure of a language determines a native speaker's perception and categorization of experience.

The **Typological Prevalence Hypothesis** comments on universal categories that exist within language. The more frequently a given way of categorizing is found in the languages of the world, the more natural it is for human cognizers and the easier it is for children to learn.

>[!idea] The Brain’s Concepts: The Role of the Sensory-Motor System in Conceptual Knowledge
>This study by George Lakoff in 2005 introduces a model on how language is structured. _Imagining and doing use a shared neural substrate_

-   Mirror neurons
    -   Imagining is a mental simulation of action or perception, using many of the same neurons as actually acting or perceiving
-   Concept Study: Grasping
    -   The meaning of grasping is derived from our ability to imagine, perceive, and perform it

**Interactionist Theory of Meaning** embodies concepts are structured by our constant encounter and interaction with the world via our bodies and brains. 

_The sensory-motor system of the brain is multimodal, not modular, just based on the circuitry of the brain._
-   Concept Study: Language
    -   Utilized many linked modalities (sight, hearing, motor actions, etc.)
    -   Theory of Cogs - _language has the ability to characterize abstract concepts_

---
## noam chomsky
Linguist responsible for the shift away from strictly behaviorist perspectives towards the internal processes that produce the behaviors we observe.

#### language acquisition
**Universal Grammar**, proposed by Noam Chomsky
-   If linguistic experience is missing during the **critical period**, language ability is permanently impaired
**The Cognition Hypothesis**, Peter Robinson
-   When learning tasks that are functionally demanding and difficult, learners are more encouraged to produce complex and accurate language

--- 
## language in the brain
**Broca’s Area** near the motor cortex of the left frontal premotor cortex is the motor speech area. This area regulates breathing patterns while speaking and vocalizations required for normal speech.
**Wernicke’s Area** in the left posterior temporal/parietal cortex is the center of comprehension for language and speech sounds.

>[!note] The Wada Test
>In most people, language is dominated in the left hemisphere. The **Wada Test** is used to determine which hemisphere a person’s language center is in. The process involves sedating one hemisphere of the brain at a time, or using fMRI to map the active regions of the brain when speaking.

*Aphasia -* impairment of language
*Apaxia -* inability to perform some task coherently (e.g. Broca’s Aphasia)
*Agnosia -* inability to process or interpret sensations (e.g. Wernicke’s Aphasia)

---
## information theoretic view
In cognitive science, [[Probability/entropy]] is a framework for analyzing communication as a means of expressing thought. The things you say are encodings of thoughts that must be deciphered by the person you speak to.

This viewpoint of words and sentences as *encodings* is actually the basis for [[natural language processing]] in machine learning.

##### natural language processing
It’s difficult for NLP models to pass the [[artificial intelligence#alan turing (1912 - 1954)|Turing test]] because
1.  Speech and action are linked — can you/may you do something?
2.  Context — she, them, it ...
3.  Shared knowledge — general knowledge, pop culture references, news
4.  Ambiguity — words with multiple meanings, run-on sentences

**GPT-3** is a _predictive language model_ from OpenAI. It fails Turing Test when given non-sensical prompts (randomly outputs an answer)