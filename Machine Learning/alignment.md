In [[artificial intelligence]], a critical challenge is ensuring that trained models and agents act in a way that aligns with human goals and values (see: the [[artificial intelligence#superintelligence and ai++|Paperclip Maximizer]]).  Thus, the field of **alignment** works to steer the behavior of intelligent systems toward a person's or group's intended goals, preferences, or ethical principles.
## rlhf
The subjectivity of generated output produced by [[large language models]] creates a challenge for evaluation. For open-ended tasks like story writing or generating ideas, writing a loss function which accurately captures "creativity" and other complex human values is intractable. **Reinforcement learning from human feedback (RLHF)** uses methods from [[reinforcement learning]] to directly optimize a language model with human feedback.

RLHF is implemented as a [[large language models#fine-tuning|fine-tuning]] method requiring multiple training stages:
1. **Pre-training**: RLHF begins with a base language model. For example, OpenAI's first popular RLHF model, InstructGPT, used GPT-3 as its base model.
2. **Training the reward model (RM)**: the RM takes in a sequence of text and returns a scalar reward representing human preferences. These LMs for reward modeling can be both another fine-tuned LM or a LM trained from scratch on the preference data. 
3. **Fine-tuning with RL**: 

#### reward model
The training dataset of prompt-generation pairs for the RM is generated by sampling a set of prompts from a predefined dataset. 
- Anthropic’s data generated primarily with a chat tool on Amazon Mechanical Turk (MTurk)
- OpenAI used prompts submitted by users to the GPT API
For the given prompts, human annotators are used to rank the generated text outputs from the LM. There are multiple methods for ranking the text. One method that has been successful is to have users compare generated text from two language models conditioned on the same prompt.
![[reward.png]]
#### fine-tuning with reinforcement learning
Fine-tuning with RL is a bit different from traditional fine-tuning, because we must re-frame the fine-tuning task as an RL problem.
- **Policy**: the policy $\pi$ is a language model that takes in prompts and returns a sequence of generated text (or just probability distributions over text)
- **Action space**: the action space of the policy is all the unique tokens in the LM's vocabulary
- **Observation space**: distribution of possible input token sequences, which is on the order of `vocab_size`^`length_of_input_sequence`. 
- **Reward function**: combination of the reward model and a constraint on policy shift

The actual update process is performed by passing in a prompt $x$ and its output $y$ generated by the current iteration of the fine-tuned policy into the trained reward model. The RM returns $r_{\theta}$, a scalar measure of the "preferability" of this prompt-generation pair.

We then impose a penalty $r_{KL}$ to prevent the RL policy's per-token probability distribution from diverging too far from the initial distribution in the pre-trained model. This is done by computing the [[information theory#kullback-leibler divergence|Kullback-Leibler (KL) divergence]] between the distributions and scaling by a $\lambda$ factor. 

The final reward used is computed as $r = r_{\theta} - \lambda r_{KL}$. We will use this to update the LM's parameters to maximize $r$. This is done using a policy-gradient RL algorithm called **proximal policy optimization (PPO)**.
##### proximal policy optimation
PPO is a trust region optimization algorithm, meaning it tries to ensure that the updated policy does not deviate too far from the current policy in a single training step. It achieves this by clipping the **policy ratio** between the new and old policy:
$$r_t(θ)=\frac{\pi_{\theta}(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)}$$
where $\pi_{\theta}$ is the current policy and $\pi_{\theta_{\text{old}}}$ is the policy before the update. $s_t$ is the sequence of tokens generated so far (at time step $t$) and $a_t$ is the action taken, or the next token generated. The objective encourages increasing the probability of actions (i.e., generated tokens) that yield high reward, but prevents large changes to the model’s behavior in a single step.

Importantly, PPO is an **on-policy algorithm**, which means that each update is performed using only the data sampled from the current version of the policy.

## weak-to-strong generalization
The problem with RLHF is that it relies on human supervision to create the feedback for the reward model. This can become expensive to obtain, and as AI systems become larger and more complex, human supervision may not be reliable.

**Weak-to-strong generalization** was presented by OpenAI as a training approach to use smaller models to supervise more powerful ones.
![[weak-to-strong.png]]